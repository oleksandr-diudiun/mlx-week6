{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (15.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow matplotlib sentencepiece pandas opencv-python\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read data from both Parquet files\n",
    "# train_0 = pd.read_parquet('../dataset/0000.parquet')\n",
    "# train_1 = pd.read_parquet('../dataset/0001.parquet')\n",
    "# train_2 = pd.read_parquet('../dataset/0002.parquet')\n",
    "# train_3 = pd.read_parquet('../dataset/0003.parquet')\n",
    "# train_4 = pd.read_parquet('../dataset/0004.parquet')\n",
    "# train_5 = pd.read_parquet('../dataset/0005.parquet')\n",
    "# train_6 = pd.read_parquet('../dataset/0006.parquet')\n",
    "# train_7 = pd.read_parquet('../dataset/0007.parquet')\n",
    "# train_8 = pd.read_parquet('../dataset/0008.parquet')\n",
    "\n",
    "# train = pd.concat([train_0, train_1, train_2, train_3, train_4, train_5, train_6, train_7, train_8,], ignore_index=True)\n",
    "# train = train.reset_index(drop=True)\n",
    "\n",
    "# print(train.shape)\n",
    "# print(train[:2])\n",
    "# train = train.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../dataset/spm_10000_vocab_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_file_path, image_side, patch_side, sp,  device, validate = False):\n",
    "        super().__init__()\n",
    "        self.sp = sp\n",
    "        if validate == True:\n",
    "            self.data = pd.read_parquet(data_file_path) [:100]\n",
    "        else:\n",
    "            self.data = pd.read_parquet(data_file_path)\n",
    "        self.dataset = []\n",
    "        self.device = device\n",
    "\n",
    "        for row in self.data.itertuples():\n",
    "            image = self.tranformImageForPatching(row.image, image_side, patch_side)\n",
    "            patches = self.patchImage(image, image_side, patch_side)\n",
    "            imageTokens = self.patchesToTokens(patches)\n",
    "\n",
    "            for caption in row.caption:\n",
    "                self.dataset.append([\n",
    "                    row.image,  \n",
    "                    imageTokens, \n",
    "                    caption,\n",
    "                ])\n",
    "    \n",
    "    def tranformImageForPatching(self, image, image_side, patch_side):\n",
    "        image_pxl = cv2.imdecode(np.frombuffer(image['bytes'], np.uint8), cv2.IMREAD_COLOR)\n",
    "        image_pxl = cv2.cvtColor(image_pxl, cv2.COLOR_BGR2RGB)\n",
    "        cropped_image = self.cropImage(image_pxl)\n",
    "        # padded_image = self.addPadding(image_pxl, patch_side)\n",
    "\n",
    "        resized_image = cv2.resize(cropped_image,  (image_side, image_side), interpolation=cv2.INTER_AREA)        \n",
    "\n",
    "        return resized_image\n",
    "    \n",
    "    def addPadding(self, image, patch_side):\n",
    "        ## For flaxible tokens amount\n",
    "        # padded_height = ((image.shape[0] // patch_side) + 1) * patch_side\n",
    "        # padded_width = ((image.shape[1] // patch_side) + 1) * patch_side\n",
    "        # padded_image = cv2.copyMakeBorder(image, 0, padded_height - image.shape[0], 0, padded_width - image.shape[1], cv2.BORDER_REFLECT)\n",
    "        \n",
    "        # Get dimensions of the image\n",
    "        height, width = image.shape[:2]\n",
    "        pad_size = max(width, height)\n",
    "        delta_w = pad_size - width\n",
    "        delta_h = pad_size - height\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        # Pad the image with black color (0, 0, 0) BORDER_CONSTANT\n",
    "        image_padded = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "        return image_padded\n",
    "    \n",
    "    def cropImage(self, image):\n",
    "          # Get the dimensions of the image\n",
    "        height, width = image.shape[:2]\n",
    "        side_length = min(height, width)\n",
    "\n",
    "        # Calculate coordinates to crop the central square\n",
    "        top = (height - side_length) // 2\n",
    "        left = (width - side_length) // 2\n",
    "        bottom = top + side_length\n",
    "        right = left + side_length\n",
    "\n",
    "        # Crop the central square\n",
    "        cropped_image = image[top:bottom, left:right]\n",
    "        return cropped_image\n",
    "    \n",
    "    def patchImage(self, image, image_side, patch_side):\n",
    "        patches = []\n",
    "        for y in range(0, image_side, patch_side):\n",
    "            for x in range(0, image_side, patch_side):\n",
    "                # Extract the block\n",
    "                block = image[y:y+patch_side, x:x+patch_side]\n",
    "                patches.append(block)\n",
    " \n",
    "        return patches\n",
    "    \n",
    "    def patchesToTokens(self, patches):\n",
    "        tokens = []\n",
    "        for patch in patches:\n",
    "            patch_tensor = torch.tensor(patch, device = self.device).permute(2, 0, 1).float()  # CHW format\n",
    "            normalized_patch = patch_tensor / 255.0\n",
    "            normalized_patch = normalized_patch.reshape(-1)\n",
    "            tokens.append(normalized_patch)\n",
    "        return torch.stack(tokens, dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        imageTokens = self.dataset[idx][1]\n",
    "        caption_input  = torch.tensor([sp.PieceToId('<s>')] + self.sp.EncodeAsIds(self.dataset[idx][2]), dtype=torch.long, device = self.device)\n",
    "        caption_target = torch.tensor(self.sp.EncodeAsIds(self.dataset[idx][2]) + [sp.PieceToId('</s>')], dtype=torch.long, device = self.device)\n",
    "        return image, imageTokens, caption_input, caption_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image, imagesTokens, captionsInputs, captionsTargets = zip(*batch)  # Unzip the batch into inputs and targets\n",
    "    inputs_padded = pad_sequence(captionsInputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(captionsTargets, batch_first=True, padding_value=0)\n",
    "    return image, torch.stack(imagesTokens, dim=0), inputs_padded, targets_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_size, head_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.head_size      = head_size\n",
    "        self.embed_size     = embed_size\n",
    "        self.device         = device\n",
    "        \n",
    "        self.Key   = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size]\n",
    "        self.Query = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size] \n",
    "        self.Value = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size] \n",
    "        \n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k , v, dont_look_ahead = True, padding_mask = None):\n",
    "        batchSize, tokens, head_size = q.shape\n",
    "        query = self.Query(q)  # Size: [batchSize x tokens x head_size]\n",
    "        key   = self.Key(k)    # Size: [batchSize x tokens x head_size]\n",
    "        value = self.Value(v)  # Size: [batchSize x tokens x head_size]\n",
    "\n",
    "        tril = torch.tril(torch.ones(tokens, tokens, device = self.device))               # Size: [tokens_amount x tokens_amount]. Diagonale ones left side only.                                                                      \n",
    "\n",
    "        # Compute Attention scores (\"Affinities\")\n",
    "        attention = query @ key.transpose(-2, -1) * head_size**0.5                        # [Batch Size x Tokens amount x head_size] @ [Batch Size x head_size x Tokens amount] --> [Batch Size x Tokens amount x Tokens amount]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            attention = attention.masked_fill(padding_mask == 0, float(-1e9))           # Size: [batchSize x tokens x tokens]\n",
    "        if dont_look_ahead == True :\n",
    "            attention = attention.masked_fill(tril[:tokens, :tokens] == 0, float(-1e9)) # Size: [batchSize x tokens x tokens]\n",
    "        \n",
    "        attention = F.softmax(attention, dim=-1)                                          # Size: [batchSize x tokens x tokens]\n",
    "        attention = self.Dropout(attention)\n",
    "        \n",
    "        out = attention @ value                                                           # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        return out                                                                        # Size: [Batch Size x Tokens Amount x head_size]\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_size, dropout, device):\n",
    "        super().__init__()  \n",
    "        self.num_heads  = num_heads\n",
    "        self.head_size  = head_size\n",
    "        self.embed_size = embed_size \n",
    "        self.device     = device\n",
    "        \n",
    "        self.Heads = nn.ModuleList()\n",
    "        for _ in range(num_heads):\n",
    "            self.Heads.append(Head(self.embed_size, self.head_size, dropout, self.device)) # ModuleList Size: [num_heads]\n",
    "\n",
    "        self.Projection = nn.Linear(self.embed_size, self.embed_size)    # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, dont_look_ahead = True, mask=None):\n",
    "        multiHead = torch.cat([head(q, k, v, dont_look_ahead, mask) for head in self.Heads], dim=-1)  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        projection = self.Dropout(self.Projection(multiHead))            # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        return projection                                                # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, attentions):\n",
    "        return self.FeedForward(attentions)\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,  embed_size, num_heads, head_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads  = num_heads\n",
    "        self.head_size  = head_size\n",
    "        self.device     = device\n",
    "\n",
    "        self.MultiAttentionHeads = MultiHeadAttention(self.embed_size, self.num_heads, self.head_size, dropout, self.device) # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.FeedForward         = FeedForward(self.embed_size, dropout)   # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln1                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln2                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln3                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "\n",
    "    def forward(self, captionPositionedEmbeddings, encoderK, encoderV, crossMask=None, mask=None):\n",
    "\n",
    "        captionQ = captionK = captionV = self.Ln1(captionPositionedEmbeddings)\n",
    "        captionAttentions  = captionPositionedEmbeddings + self.MultiAttentionHeads(captionQ, captionK, captionV, dont_look_ahead = True, mask=mask) # Size: [Batch Size x Tokens Amount x embed_size]. Apply MultiHead Attention Layer\n",
    "\n",
    "        decoderQ = self.Ln2(captionAttentions)\n",
    "        encoderK = self.Ln2(encoderK) \n",
    "        encoderV = self.Ln2(encoderV)\n",
    "\n",
    "        mergedAttentions  = decoderQ + self.MultiAttentionHeads(decoderQ, encoderK, encoderV, dont_look_ahead = False, mask=crossMask)\n",
    "        feedForward = mergedAttentions + self.FeedForward(self.Ln3(mergedAttentions))       # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        return feedForward                                                                  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,  embed_size, num_heads, head_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads  = num_heads\n",
    "        self.head_size  = head_size\n",
    "        self.device     = device\n",
    "\n",
    "        self.MultiAttentionHeads = MultiHeadAttention(self.embed_size, self.num_heads, self.head_size, dropout, self.device) # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.FeedForward         = FeedForward(self.embed_size, dropout)   # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln1                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln2                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "\n",
    "    def forward(self, positionedEmbeddings, mask=None):\n",
    "        imageQ = imageK = imageV  = self.Ln1(positionedEmbeddings)\n",
    "        attentions  = positionedEmbeddings + self.MultiAttentionHeads(imageQ, imageK, imageV, dont_look_ahead = False, mask=None) # Size: [Batch Size x Tokens Amount x embed_size]. Apply MultiHead Attention Layer\n",
    "        feedForward = attentions + self.FeedForward(self.Ln2(attentions))                                                         # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        return feedForward           \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads, embed_size, head_size, image_side, patch_side, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device                 = device\n",
    "        self.embed_size             = embed_size\n",
    "        self.num_blocks             = num_blocks\n",
    "        self.num_heads              = num_heads\n",
    "        self.head_size              = head_size\n",
    "        self.image_side             = image_side\n",
    "        self.patch_side             = patch_side\n",
    "        \n",
    "        pathcesEmbeddingInputSize = patch_side**2 * 3\n",
    "        self.pathcesEmbedding = nn.Linear(pathcesEmbeddingInputSize, embed_size)\n",
    "        self.EncoderBlocks = nn.ModuleList([\n",
    "            EncoderBlock(self.embed_size, self.num_heads, self.head_size, dropout, self.device) for _ in range(self.num_blocks)\n",
    "        ])\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_size)\n",
    "    \n",
    "    def positionEncoding(self, input_tokens_amount):\n",
    "        positionEncoding = torch.zeros(input_tokens_amount, self.embed_size, device = self.device)                                       # max length x embedding dimmensions equivalent to Size: [input_tokens_amount x embed_size]\n",
    "        positions = torch.arange(0, input_tokens_amount, dtype=torch.float, device = self.device).unsqueeze(1)                           # Tensor [0, 1, 2,..., input_tokens_amount] -> [⋮] : rotated for each value in separate row of 1 column\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_size, 2, device = self.device).float() * (-math.log(10000.0) / self.embed_size)) # Tensor [0, 2, 4,..., embed_size] x (-math.log(10000.0) / self.embed_size) --> exponenta\n",
    "\n",
    "        positionEncoding[:, 0::2] = torch.sin(positions * div_term)        # Size: [input_tokens_amount x embed_size], set the odd values (columns 1 and 3) \n",
    "        positionEncoding[:, 1::2] = torch.cos(positions * div_term)        # Size: [input_tokens_amount x embed_size], set the even values (columns 2 and 4) \n",
    " \n",
    "        return positionEncoding.unsqueeze(0)                               # Size: [1 (for batch dim) x input_tokens_amount x embed_size]\n",
    "     \n",
    "    def forward(self, imagesTokens):\n",
    "        # imagesTokens   Size: [Batch Size x Image Tokens Amount x patch_side**2 * 3]       \n",
    "        \n",
    "        batchSize, imageTokensAmount, _ = imagesTokens.shape\n",
    "        imageEmbedding = self.pathcesEmbedding(imagesTokens)                                   # Size [Batch Size  x Image Tokens Amount x embed_size]\n",
    "        imagePositionedEmbeddings = imageEmbedding + self.positionEncoding(imageTokensAmount)  # Size [Batch Size  x Image Tokens Amount x embed_size]\n",
    "\n",
    "        imageBlocks = imagePositionedEmbeddings\n",
    "        for block in self.EncoderBlocks:\n",
    "            imageBlocks = block(imageBlocks, mask = None)  # Size: [Batch Size x Image Tokens Amount x embed_size]\n",
    "        encoderOut = self.final_layer_norm(imageBlocks)    # Size: [Batch Size x Image Tokens Amount x embed_size]\n",
    "\n",
    "        return encoderOut                                  # Size: [Batch Size x Image Tokens Amount x embed_size]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads, embed_size, head_size,  vocab_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device                 = device\n",
    "        self.num_blocks             = num_blocks\n",
    "        self.embed_size             = embed_size\n",
    "        self.vocab_size             = vocab_size\n",
    "        self.num_heads              = num_heads\n",
    "        self.head_size              = head_size\n",
    "\n",
    "        self.captionEmbedding = torch.nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embed_size, device = self.device) # Size: [vocab_size x embed_size]\n",
    "        self.DecoderBlocks = nn.ModuleList([\n",
    "            DecoderBlock(self.embed_size, self.num_heads, self.head_size, dropout, self.device) for _ in range(self.num_blocks)\n",
    "        ])\n",
    "        self.decoder_final_layer_norm = nn.LayerNorm(self.embed_size)\n",
    "        self.LangModelHead  = nn.Linear(self.embed_size, self.vocab_size, device = self.device) # Size: [embed_size x vocab_size]\n",
    "    \n",
    "    def positionEncoding(self, input_tokens_amount):\n",
    "        positionEncoding = torch.zeros(input_tokens_amount, self.embed_size, device = self.device)                                  # max length x embedding dimmensions equivalent to Size: [input_tokens_amount x embed_size]\n",
    "        positions = torch.arange(0, input_tokens_amount, dtype=torch.float, device = self.device).unsqueeze(1)                      # Tensor [0, 1, 2,..., input_tokens_amount] -> [⋮] : rotated for each value in separate row of 1 column\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_size, 2, device = self.device).float() * (-math.log(10000.0) / self.embed_size)) # Tensor [0, 2, 4,..., embed_size] x (-math.log(10000.0) / self.embed_size) --> exponenta\n",
    "\n",
    "        positionEncoding[:, 0::2] = torch.sin(positions * div_term)        # Size: [input_tokens_amount x embed_size], set the odd values (columns 1 and 3) \n",
    "        positionEncoding[:, 1::2] = torch.cos(positions * div_term)        # Size: [input_tokens_amount x embed_size], set the even values (columns 2 and 4) \n",
    " \n",
    "        return positionEncoding.unsqueeze(0)                               # Size: [1 (for batch dim) x input_tokens_amount x embed_size]\n",
    "     \n",
    "    def forward(self, captionsInputs, encoderK, encoderV):\n",
    "        # captionsInputs Size: [Batch Size x Tokens Amount]\n",
    "    \n",
    "        batchSize, captionTokensAmount = captionsInputs.shape\n",
    "\n",
    "        caption_padding_lables = (captionsInputs != 0).float().to(device)\n",
    "        caption_padding_mask = caption_padding_lables.unsqueeze(-1) @ caption_padding_lables.unsqueeze(-2)\n",
    "\n",
    "        batchSize, patchesAmount, _ = encoderK.shape\n",
    "        \n",
    "        image_padding_label = torch.ones(batchSize, patchesAmount, dtype=torch.float).to(device)\n",
    "\n",
    "        cross_padding_mask = caption_padding_lables.unsqueeze(-1) @ image_padding_label.unsqueeze(-2)\n",
    "\n",
    "        captionEmbeddings = self.captionEmbedding(captionsInputs)                              # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        positionedEmbeddings = captionEmbeddings + self.positionEncoding(captionTokensAmount)  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        \n",
    "        decoderInput = positionedEmbeddings\n",
    "        for block in self.DecoderBlocks:\n",
    "            decoderInput = block(decoderInput, encoderK, encoderV, crossMask = cross_padding_mask, mask = caption_padding_mask)        # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        decoderOut = self.decoder_final_layer_norm(decoderInput)\n",
    "        \n",
    "        logits = self.LangModelHead(decoderOut)                            # Size: [Batch Size x Tokens Amount x vocab_size]\n",
    "        return logits                                                      # Size: [Batch Size x Tokens Amount x vocab_size]\n",
    "\n",
    "class captionTransformer(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads, embed_size, head_size, vocab_size, image_side, patch_side, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device                 = device\n",
    "        self.num_blocks             = num_blocks\n",
    "        self.embed_size             = embed_size\n",
    "        self.vocab_size             = vocab_size\n",
    "        self.num_heads              = num_heads\n",
    "        self.head_size              = head_size\n",
    "        self.image_side             = image_side\n",
    "        self.patch_side             = patch_side\n",
    "        \n",
    "        self.Encoder = Encoder(num_blocks, num_heads, embed_size, head_size, image_side, patch_side, dropout, device)\n",
    "        self.Decoder = Decoder(num_blocks, num_heads, embed_size, head_size, vocab_size, dropout, device)\n",
    "      \n",
    "    def forward(self, imagesTokens, captionsInputs):\n",
    "        # captionsInputs Size: [Batch Size x Tokens Amount]\n",
    "        # imagesTokens   Size: [Batch Size x Image Tokens Amount x patch_side**2 * 3]       \n",
    "        \n",
    "        # print(\"Encoder:\")\n",
    "        encoderK = encoderV = self.Encoder(imagesTokens)            # Size: [Batch Size x Image Tokens Amount x embed_size]\n",
    "        # print(\"Decoder:\")\n",
    "        logits = self.Decoder(captionsInputs, encoderK, encoderV)  # Size: [Batch Size x Caption Tokens Amount x vocab_size]\n",
    "        return logits                                               # Size: [Batch Size x Caption Tokens Amount x vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Embedd = nn.Embedding(10, 2)\n",
    "# input_indices = torch.tensor([\n",
    "#     [1, 1, 1, 1],\n",
    "#     [1, 1, 1, 0],  # Using '0' as padding\n",
    "#     [1, 0, 0, 0]   # More padding examples\n",
    "# ])\n",
    "\n",
    "# print(\"Input:\\n\",input_indices.shape)\n",
    "# print(input_indices, \"\\n\")\n",
    "\n",
    "# padding_lable = (input_indices != 0).int()\n",
    "# print(\"Padding mask:\\n\", padding_lable.shape)\n",
    "# print(padding_lable, \"\\n\")\n",
    "\n",
    "# padding_mask_A = padding_lable.unsqueeze(-1)\n",
    "# print(\"unsqueeze(-1) Padding mask:\\n\", padding_mask_A.shape)\n",
    "# padding_mask_B = padding_lable.unsqueeze(-2)\n",
    "# print(\"unsqueeze(-2) Padding mask:\\n\", padding_mask_B.shape)\n",
    "\n",
    "# padding_mask = padding_mask_A @ padding_mask_B\n",
    "\n",
    "# print(\"Dot product Padding mask:\\n\", padding_mask.shape)\n",
    "# print(padding_mask, \"\\n\")\n",
    "\n",
    "# embedded = Embedd(input_indices)\n",
    "# print(\"embedded input:\\n\", embedded.shape)\n",
    "# # print(embedded, \"\\n\")\n",
    "\n",
    "# attention = embedded @ embedded.transpose(-2, -1)\n",
    "# print(\"embedded.transpose(-2, -1) :\\n\",embedded.transpose(-2, -1).shape)\n",
    "# print(\"attention :\\n\",attention.shape)\n",
    "# print(attention)\n",
    "\n",
    "# attention = attention.masked_fill(padding_mask == 0, float('-inf')) \n",
    "# print(\"padded attention :\\n\", attention.shape)\n",
    "# print(attention)\n",
    "# soft  = F.softmax(attention, dim=-1) \n",
    "# print(\"padded attention soft :\\n\", soft.shape)\n",
    "# print(soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(torch.tril(torch.ones(2, 2)))\n",
    "\n",
    "# p = torch.tensor([1,1,0,0]).float()\n",
    "# p1 = p.unsqueeze(-1)\n",
    "# p2 = p.unsqueeze(-2)\n",
    "# print(p)\n",
    "# print(p1)\n",
    "# print(p2)\n",
    "# mask = p1 @ p2\n",
    "# print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_size:  64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "dropout = 0.2\n",
    "maxNewTokens = 100\n",
    "vocab_size = sp.GetPieceSize()\n",
    "embed_size = 512 # 512\n",
    "num_blocks = 6\n",
    "num_heads = 8 # 8\n",
    "head_size = int(embed_size / num_heads)\n",
    "print(\"head_size: \", head_size)\n",
    "if embed_size % num_heads != 0:\n",
    "    print(\"embed_size Cannot be divided evenly by num_heads.\")\n",
    "    sys.exit()\n",
    "image_side = 256\n",
    "patch_side = 16\n",
    "if image_side % patch_side != 0:\n",
    "    print(\"image_side Cannot be divided evenly by patch_side.\")\n",
    "    sys.exit()\n",
    "\n",
    "data = [\n",
    "    # '../dataset/0000.parquet',\n",
    "    # '../dataset/0001.parquet',\n",
    "    # '../dataset/0002.parquet',\n",
    "    '../dataset/0003.parquet',\n",
    "    '../dataset/0004.parquet',\n",
    "    # '../dataset/0005.parquet',\n",
    "    # '../dataset/0006.parquet',\n",
    "    # '../dataset/0007.parquet',\n",
    "]\n",
    "\n",
    "validation_data = [\n",
    "    '../dataset/0001.parquet',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = captionTransformer(\n",
    "    embed_size          = embed_size, \n",
    "    num_blocks          = num_blocks,\n",
    "    num_heads           = num_heads,\n",
    "    head_size           = head_size, \n",
    "    vocab_size          = vocab_size,\n",
    "    image_side          = image_side,\n",
    "    patch_side          = patch_side,\n",
    "    dropout             = dropout,\n",
    "    device              = device,\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File started :  ../dataset/0003.parquet 2024-04-12 14:00:57.012237\n",
      "Preprocessing finished:  2024-04-12 14:03:15.393540\n",
      "File started :  ../dataset/0004.parquet 2024-04-12 14:03:15.393784\n",
      "Preprocessing finished:  2024-04-12 14:05:26.083548\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "datasets = []\n",
    "for file in data:\n",
    "    print(\"File started : \", file, datetime.datetime.now())\n",
    "    captionDataset = CaptionDataset(file, image_side, patch_side, sp, device)\n",
    "    datasets.append(captionDataset)\n",
    "    print(\"Preprocessing finished: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2024-04-12 14:05:26.102089\n",
      "Dataloader started :  2024-04-12 14:05:26.107727\n",
      "Dataloader finished:  2024-04-12 14:05:26.108140\n",
      "[1, 10] loss: 9.095\n",
      "[1, 20] loss: 8.614\n",
      "[1, 30] loss: 8.210\n",
      "[1, 40] loss: 7.836\n",
      "[1, 50] loss: 7.598\n",
      "[1, 60] loss: 7.232\n",
      "[1, 70] loss: 7.141\n",
      "[1, 80] loss: 6.852\n",
      "[1, 90] loss: 6.776\n",
      "[1, 100] loss: 6.537\n",
      "[1, 110] loss: 6.445\n",
      "[1, 120] loss: 6.320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 15\n",
    "print(\"Training started at:\", datetime.datetime.now())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for ds in datasets:\n",
    "        print(\"Dataloader started : \", datetime.datetime.now())\n",
    "        captionDataloader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn,)\n",
    "        print(\"Dataloader finished: \", datetime.datetime.now())\n",
    "        for batch_idx, (image, imagesTokens, captionsInputs, captionsTargets) in enumerate(captionDataloader):\n",
    "            logits  = model(imagesTokens, captionsInputs)                     # Size: [Batch Size x Caption Tokens Amount x vocab_size]\n",
    "            \n",
    "            batchSize, captionTokensAmount, vocabSize = logits.shape\n",
    "            \n",
    "            logits  = logits.view(batchSize * captionTokensAmount, vocabSize) # Size: [(Batch Size * captionTokensAmount) x Vocab Size]\n",
    "            targets = captionsTargets.view(batchSize * captionTokensAmount)   # Size: [(Batch Size * captionTokensAmount)]\n",
    "            \n",
    "            mask = targets != 0  # Assuming -1 is used for padding in labels\n",
    "            loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "            loss = loss * mask.view(batchSize * captionTokensAmount).float()\n",
    "            loss = loss.sum() / mask.sum()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # loss = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad(set_to_none = True)\n",
    "            # optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch_idx+1) % 10 == 0:  # Print loss every 100 batches\n",
    "                print(f'[{epoch + 1}, {batch_idx + 1}] loss: {running_loss / 10:.3f}')\n",
    "                running_loss = 0.0\n",
    "                # print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for file in validation_data:\n",
    "        print(\"Eval started : \", file, datetime.datetime.now())\n",
    "        valid_captionDataset = CaptionDataset(file, image_side, patch_side, sp, device, True)\n",
    "        valid_captionDataloader = DataLoader(valid_captionDataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn,)\n",
    "        print(\"Eval preprocess finished : \", datetime.datetime.now())\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():  # No gradient is needed for validation\n",
    "            for batch_idx, (image, imagesTokens, captionsInputs, captionsTargets) in enumerate(valid_captionDataloader):\n",
    "                logits  = model(imagesTokens, captionsInputs)                     # Size: [Batch Size x Caption Tokens Amount x vocab_size]\n",
    "            \n",
    "                batchSize, captionTokensAmount, vocabSize = logits.shape\n",
    "                \n",
    "                logits  = logits.view(batchSize * captionTokensAmount, vocabSize) # Size: [(Batch Size * captionTokensAmount) x Vocab Size]\n",
    "                targets = captionsTargets.view(batchSize * captionTokensAmount)   # Size: [(Batch Size * captionTokensAmount)]\n",
    "                \n",
    "                mask = targets != 0  # Assuming -1 is used for padding in labels\n",
    "                loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "                loss = loss * mask.view(batchSize * captionTokensAmount).float()\n",
    "                loss = loss.sum() / mask.sum()\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        val_loss = val_running_loss / len(valid_captionDataloader)\n",
    "        print(f'Epoch {epoch + 1} validation loss: {val_loss:.3f}')\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    for file in validation_data:\n",
    "        valid_captionDataset = CaptionDataset(file, image_side, patch_side, sp, device, True)\n",
    "        for attempt in range(5):\n",
    "            random_index = random.randint(0, len(valid_captionDataset) - 1)\n",
    "            image, imagesTokens, captionsInputs, captionsTargets = valid_captionDataset[random_index]\n",
    "            \n",
    "            nparr = np.frombuffer(image['bytes'], np.uint8)\n",
    "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "            plt.axis('off')  # Hide axis\n",
    "            plt.show()\n",
    "    \n",
    "            for J in range(3):\n",
    "                startCaption = \"<s>\"\n",
    "                startTokensIds = sp.EncodeAsIds(startCaption) \n",
    "                startTokensTensor = torch.tensor(startTokensIds, dtype=torch.long, device = device).unsqueeze(0)   # [1 x int, ..., tokens_length] \n",
    "                finalCaptionTokensIds = startTokensIds\n",
    "                \n",
    "                for i in range(maxNewTokens):\n",
    "    \n",
    "                    genLogits  = model(imagesTokens.unsqueeze(0), startTokensTensor)               # Size: [Batch Size x Caption Tokens Amount x vocab_size]\n",
    "                    # Let's focus only on last token in sequence\n",
    "                    genLogits = genLogits[:, -1, :]                                 # Size: [Batch Size x Vocab Size]  \n",
    "                    probabilities = F.softmax(genLogits, dim=-1)                    # Size: [Batch Size x Vocab Size], Probavilities of each word from vocab\n",
    "                    nextIdx = torch.multinomial(probabilities, num_samples = 1)     # Size: [Batch Size x 1]\n",
    "                    \n",
    "                    # apply running index to the running sequence \n",
    "                    startTokensTensor = torch.cat((startTokensTensor, nextIdx), dim=1) # Size: [Batch Size x (Tokens Amount + 1)]\n",
    "                    finalCaptionTokensIds.append(nextIdx.item())\n",
    "                    finalStoryTokens = []\n",
    "                    for tokenId in finalCaptionTokensIds:\n",
    "                        finalStoryTokens.append(sp.IdToPiece(tokenId))\n",
    "               \n",
    "                finalStory = ''.join(finalStoryTokens).replace('▁', ' ').strip()  # Assuming '▁' is the SentencePiece underline character\n",
    "                print(\"Caption #\", J, \": \", finalStory, \"\\n\\n\")\n",
    "\n",
    "       \n",
    "                \n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
